# Local LLM Advice Evaluation  
**CS 383 Final Project – Spring 2025**

This project evaluates the ability of a **fully local large language model (GPT-2)** to generate helpful responses to real-life advice questions. We compare LLM responses to human-written answers using semantic similarity with Sentence-BERT.

---
## Important Notes

⚠️ **DISCLAIMER**  
This script uses real prompts scraped from Reddit's r/Advice. As such, `scored_advice_responses.csv` may contain sensitive, disturbing, or sexual content. Use and review responsibly.

- This project opted for **full local inference** rather than using the provided GPT API. This allowed complete control over evaluation and reproducibility, while still meeting all core project objectives. All LLM inference is run locally using Hugging Face's `transformers` library.

- The project initially explored using Google's Gemma 2B Instruct model. However, due to strict usage restrictions and hardware limitations (Gemma requires significant VRAM and CPU resources), we decided not to proceed with it. Although the model was downloaded after agreeing to Google's license terms, it was not used or fine-tuned.  
**All experiments, results, and deliverables are based on GPT-2.**

---

## Final Project Report

- **Team:** Group 2 (aka CAS)  
- **GitHub Repository:** https://github.com/cerenoguz1/llm-reddit-advice-eval  
- **Google Drive Folder:** https://drive.google.com/drive/folders/1hSICHUP55ftErAFduyBWBGZPxUj5qEdA?usp=sharing  
 **Download-only requirement:** `checkpoint-224.zip` (fine-tuned model)

### Google Drive Contents

| File                          | Description                                |
|-------------------------------|--------------------------------------------|
| llm-reddit-advice-eval.zip    | Complete inference and evaluation scripts  |
| checkpoint-224.zip            | Fine-tuned GPT-2 model checkpoint          |
| gpt2-finetune-full-backup.zip | Full fine-tuning code, data, outputs       |
| requirements.txt              | Python environment for reproducibility     |

---

## 1. Problem Statement

**Goal:**  
Evaluate whether a fine-tuned GPT-2 model can generate meaningful, helpful advice in response to real-world prompts from Reddit's r/Advice forum.

**Inputs and Outputs:**  
- **Input:** A text prompt containing a personal advice question  
- **Output:** A natural language response generated by GPT-2

**Course Objectives Met:**  
- Language model fine-tuning  
- Local model inference  
- Semantic similarity-based evaluation

---

## 2. Dataset

**Name:** Reddit Advice Dataset (self-collected)  
**Source:** Scraped from Reddit's r/Advice using Reddit API + manual curation

**Stats:**  
- ~5,000 prompt-response pairs  
- Average prompt length: ~100 tokens  
- Average response length: ~80 tokens

**Processing Steps:**  
- Cleaned and filtered  
- Converted to Hugging Face format via `preprocess.py`  
- Randomly split: 4,500 train / 500 test

---

## 3. Prompt Methodology

**Prompt Format Used for Training & Inference:**  
```
Prompt: <user advice question>\nAdvice:
```

**Example:**  
- **Input:**  
  `Prompt: My prom dress got ruined the day of the event. What should I do?\nAdvice:`  
- **Generated Output:**  
  `If you're not sure what to wear, try wearing a pair of jeans or something similar...`

**Sampling Parameters:**  
- `max_new_tokens=100`  
- `temperature=0.8`  
- `top_p=0.95`  
- `do_sample=True`  
- `pad_token_id=50256` (eos_token)

**API Use:** All inference is local using `transformers` and `Sentence-BERT`.

---

## 4. Evaluation Approach

**Metric Used:**  
- Cosine Similarity using Sentence-BERT (`all-MiniLM-L6-v2`)

**Steps:**  
1. Generate responses for test prompts using fine-tuned GPT-2  
2. Compare generated advice to ground-truth using Sentence-BERT  
3. Save scores to `scored_advice_responses.csv`

**Strengths:**  
- Full offline reproducibility  
- No reliance on external APIs

**Weaknesses:**  
- GPT-2 may struggle with emotional nuance or sensitive content

---

## 5. Results

**Similarity Scores:**  
- **Highest:** 0.506  
- **Lowest:** -0.046  
- **Average:** 0.1799

---

## 6. Feedback and Communication

- Uploaded all code and checkpoints to GitHub/Drive  
- Provided `test_one_prompt.py` for instructors  
- Added more structured documentation in the README

---

## 7. Team Member Contributions

**Ceren Oguz**  
- Preprocessed and formatted dataset  
- Fine-tuned GPT-2 using Hugging Face Trainer  
- Built Sentence-BERT scoring pipeline  
- Managed GitHub/Drive backups and documentation  
- Wrote test scripts and examples

---

## Summary

This project fine-tunes GPT-2 on a custom dataset of real Reddit r/Advice prompts. It uses local inference and Sentence-BERT cosine similarity to evaluate how semantically similar LLM responses are to human-written advice.

---

## Project Structure

```
llm-reddit-advice-eval/
├── generate_and_score_local.py        # Generates advice + scores similarity
├── test_one_prompt.py                 # Test a single prompt
├── analyze_results.py                 # Optional score analysis
├── preprocess.py                      # Dataset formatter
├── formatted.jsonl                    # Final dataset
├── reddit_advice_dataset.csv         # Raw dataset
├── scored_advice_responses.csv       # Final output
├── myenv/                             # Ignored virtual environment
```

---

## Model Details

- **Base Model:** GPT-2 from Hugging Face  
- **Trained On:** Reddit advice dataset (`formatted.jsonl`)  
- **Inference:** Local (offline, no API)  
- **Scoring:** Cosine similarity via Sentence-BERT (`all-MiniLM-L6-v2`)

---

## How to Run

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Unzip the model
unzip checkpoint-224.zip

# 3. Run evaluation
python generate_and_score_local.py

# 4. (Optional) Try a single query
python test_one_prompt.py
```
