Local LLM Advice Evaluation  
CS 383 Final Project – Spring 2025

This project evaluates the ability of a **fully local large language model** (Gemma 2B Instruct) to generate helpful responses to real-life advice questions. We compare LLM responses to human-written answers using semantic similarity with Sentence-BERT.

⚠️ DISCLAIMER:
This script uses real prompts scraped from Reddit's r/Advice. As such, 'scored_advice_responses.csv' may contain sensitive, disturbing, or sexual content. Use and review responsibly.

Final Project Report
Team: 2 aka CAS
GitHub Repository: https://github.com/cerenoguz1/llm-reddit-advice-eval

1. Problem Statement
Goal:
The goal of our project is to evaluate whether a fine-tuned GPT-2 model can generate meaningful, helpful advice in response to real-world prompts sourced from Reddit's r/Advice forum.
Inputs and Outputs:
Input: A text prompt containing a personal advice question
Output: A natural language response generated by GPT-2 providing advice
Connection to Requirements:
This project satisfies the course goals by applying language model fine-tuning and evaluation, exploring model inference offline, and comparing model outputs to human-written responses using semantic similarity metrics.

2. Dataset
Dataset Name and Source:
Name: Reddit Advice Dataset (self-collected)
Source: Scraped from Reddit's r/Advice forum using Reddit API and manual curation
Dataset Statistics:
~5,000 prompt-response pairs
Each entry contains: prompt, question, and suggestion
Average prompt length: ~100 tokens
Average suggestion length: ~80 tokens
Dataset Creation or Changes:
Cleaned and filtered for quality and length
Saved as formatted.jsonl using preprocess.py to convert to Hugging Face DatasetDict format
Split into train/test with random sampling (e.g., train: 4,500, test: 500)

3. Prompt Methodology
Prompt Template:
The prompt format used for fine-tuning and inference:
Prompt: <user advice question>\nAdvice:
Sample Input/Output Example:
Input:
Prompt: My prom dress got ruined the day of the event. What should I do?\nAdvice:
Generated Output:
If you're not sure what to wear, try wearing a pair of jeans or something similar... Also make yourself comfortable and relaxed...
Sampling Parameters:
max_new_tokens=100
temperature=0.8
top_p=0.95
do_sample=True
pad_token_id=50256 (eos_token)
API Call Description:
No API was used. All model inference was done locally using transformers and Sentence-BERT models.

4. Evaluation Approach
Metrics Used:
Cosine Similarity (Sentence-BERT): Measures semantic closeness between LLM output and ground-truth suggestion.
Evaluation Process:
Generate responses for all test prompts using fine-tuned GPT-2
Use sentence-transformers (e.g., all-MiniLM-L6-v2) to compute similarity between generated and real advice
Save to scored_advice_responses.csv
Strengths and Weaknesses:
Strengths: Fully local and reproducible, no reliance on APIs
Weaknesses: GPT-2 struggles with complex or sensitive prompts

5. Results
Summary of Metrics:
Highest similarity: 0.506
Lowest similarity: -0.046
Average similarity: 0.1799
Discussion:
The results show that GPT-2 is capable of generating advice that is occasionally semantically similar to human responses. However, the average similarity remains low, suggesting room for improvement in reasoning, emotional intelligence, and coherence.

6. Feedback and Communication
Feedback Received:
Ensure full reproducibility without APIs
Clarify where models are stored and how to load them
Add more structured examples in README
Addressed It By:
All data and models are now in Drive/GitHub with instructions
Provided test script (test_one_prompt.py) for easy validation
Added more output examples and detailed explanations in README.md

7. Team Member Contributions
Ceren Oguz:
Implemented dataset preprocessing and formatting script
Fine-tuned GPT-2 using Hugging Face Trainer
Designed the evaluation pipeline using Sentence-BERT
Handled all Git/Drive/backup workflows for reproducibility
Wrote full documentation, testing scripts, and inference demos

This project fine-tunes a GPT-2 language model on a custom dataset of real Reddit r/Advice questions and suggestions. It evaluates model-generated advice using semantic similarity (Sentence-BERT) against human-written answers.

Links
* GitHub Repository: https://github.com/cerenoguz1/llm-reddit-advice-eval
* Google Drive Backup Folder: llm-reddit-advice-eval-extra and gpt2
Google Drive Contents:
File	Description
llm-reddit-advice-eval.zip	Complete inference and evaluation scripts
checkpoint-224.zip	Fine-tuned GPT-2 model checkpoint
gpt2-finetune-full-backup.zip	Full fine-tuning code, data, outputs
requirements.txt	Exact Python environment for reproducibility
Project Structure
llm-reddit-advice-eval/
├── generate_and_score_local.py        # Generates advice + scores similarity
├── test_one_prompt.py                 # Test single advice query
├── analyze_results.py                 # (Optional) Score analysis tool
├── preprocess.py                      # Prepares JSONL dataset
├── formatted.jsonl                    # Final dataset for GPT-2
├── reddit_advice_dataset.csv         # Raw prompt-suggestion pairs
├── scored_advice_responses.csv       # Final results with similarity
├── myenv/                             # (Ignored) Local virtualenv

Model Details
* Base Model: GPT-2 from Hugging Face
* Fine-Tuned On: Reddit r/Advice dataset (formatted.jsonl)
* Inference: Local using transformers (no API or huggingface-cli)
* Similarity Scoring: Sentence-BERT (all-MiniLM-L6-v2) cosine similarity

How to Run
1. Install dependencies: pip install -r requirements.txt
2. Run full evaluation: python generate_and_score_local.py
3. Try a single query: python test_one_prompt.py
